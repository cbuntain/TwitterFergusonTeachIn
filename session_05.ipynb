{
 "metadata": {
  "name": "",
  "signature": "sha256:37f9a4f8ceb7d6f625a7b3e04d1647a53308b46220ed2c9cb9ccabf0d170a466"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import time\n",
      "import calendar\n",
      "import codecs\n",
      "import datetime\n",
      "import json\n",
      "import sys\n",
      "import gzip\n",
      "import string\n",
      "import glob\n",
      "import os\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if ( sys.version_info.major == 3 ):\n",
      "    from functools import reduce"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Module 0: Reading Tweets\n",
      "\n",
      "The first thing we do is read in tweets from a directory of compressed files. Our collection of compressed tweets are in the data_files/twitter directory, so we'll use pattern matching (called \"globbing\") to find all the tweet files in the given directory.\n",
      "\n",
      "Then, for each file, we'll open it, read each line (which is a tweet in JSON form), and build an object out of it. As part of this process, we will extract each tweet's post time and create a map from minute timestamps to the tweets posted during that minute."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweetPath = os.path.join(\"data_files\", \"twitter\")\n",
      "tweetFiles = {\n",
      "   \"time01\": os.path.join(tweetPath, \"statuses.*.gz\")\n",
      "}\n",
      "\n",
      "frequencyMap = {}\n",
      "globalTweetCounter = 0\n",
      "\n",
      "timeFormat = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
      "\n",
      "reader = codecs.getreader(\"utf-8\")\n",
      "\n",
      "for (key, path) in tweetFiles.items():\n",
      "    localTweetList = []\n",
      "    for filePath in glob.glob(path):\n",
      "        print (\"Reading File:\", filePath)\n",
      "        \n",
      "        for line in gzip.open(filePath, 'rb'):\n",
      "\n",
      "            # Try to read tweet JSON into object\n",
      "            tweetObj = None\n",
      "            try:\n",
      "                tweetObj = json.loads(reader.decode(line)[0])\n",
      "            except Exception as e:\n",
      "                continue\n",
      "\n",
      "            # Deleted status messages and protected status must be skipped\n",
      "            if ( \"delete\" in tweetObj.keys() or \"status_withheld\" in tweetObj.keys() ):\n",
      "                continue\n",
      "\n",
      "            # Try to extract the time of the tweet\n",
      "            try:\n",
      "                currentTime = datetime.datetime.strptime(tweetObj['created_at'], timeFormat)\n",
      "            except:\n",
      "                print (line)\n",
      "                raise\n",
      "\n",
      "            currentTime = currentTime.replace(second=0)\n",
      "            \n",
      "            # Increment tweet count\n",
      "            globalTweetCounter += 1\n",
      "            \n",
      "            # If our frequency map already has this time, use it, otherwise add\n",
      "            if ( currentTime in frequencyMap.keys() ):\n",
      "                timeMap = frequencyMap[currentTime]\n",
      "                timeMap[\"count\"] += 1\n",
      "                timeMap[\"list\"].append(tweetObj)\n",
      "            else:\n",
      "                frequencyMap[currentTime] = {\"count\":1, \"list\":[tweetObj]}\n",
      "\n",
      "# Fill in any gaps\n",
      "times = sorted(frequencyMap.keys())\n",
      "firstTime = times[0]\n",
      "lastTime = times[-1]\n",
      "thisTime = firstTime\n",
      "\n",
      "timeIntervalStep = datetime.timedelta(0, 60)    # Time step in seconds\n",
      "while ( thisTime <= lastTime ):\n",
      "    if ( thisTime not in frequencyMap.keys() ):\n",
      "        frequencyMap[thisTime] = {\"count\":0, \"list\":[]}\n",
      "        \n",
      "    thisTime = thisTime + timeIntervalStep\n",
      "\n",
      "print (\"Processed Tweet Count:\", globalTweetCounter)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "# Module 1: Simple Frequency Analysis\n",
      "\n",
      "In this section, we will cover a few simple analysis techniques to garner some small insights rapidly.\n",
      "\n",
      "- Twitter Timeline\n",
      "- Top Twitter Users\n",
      "- Twitter API\n",
      "- Posting Frequency Distribution\n",
      "- Popular Hashtags\n",
      "- Simple Event Detection\n",
      "- Language Distributions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Twitter Timeline \n",
      "\n",
      "To build a timeline of Twitter usage, we can simply plot the number of tweets posted per minute."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(18.5,10.5)\n",
      "\n",
      "plt.title(\"Tweet Frequency\")\n",
      "\n",
      "# Sort the times into an array for future use\n",
      "sortedTimes = sorted(frequencyMap.keys())\n",
      "\n",
      "# What time span do these tweets cover?\n",
      "print (\"Time Frame:\", sortedTimes[0], sortedTimes[-1])\n",
      "\n",
      "# Get a count of tweets per minute\n",
      "postFreqList = [frequencyMap[x][\"count\"] for x in sortedTimes]\n",
      "\n",
      "# We'll have ticks every thirty minutes (much more clutters the graph)\n",
      "smallerXTicks = range(0, len(sortedTimes), 30)\n",
      "plt.xticks(smallerXTicks, [sortedTimes[x] for x in smallerXTicks], rotation=90)\n",
      "\n",
      "# Plot the post frequency\n",
      "ax.plot(range(len(frequencyMap)), [x if x > 0 else 0 for x in postFreqList], color=\"blue\", label=\"Posts\")\n",
      "ax.grid(b=True, which=u'major')\n",
      "ax.legend()\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Top Twitter Users\n",
      "\n",
      "Ferguson was a contentiuous topic, and many people had differing opinions about the issue. Given the volume of tweets we are analyzing, we can now answer who the \"loudest\" voices were during this time. \n",
      "\n",
      "That is, who was tweeting the most during this particular time span?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create maps for holding counts and tweets for each user\n",
      "globalUserCounter = {}\n",
      "globalUserMap = {}\n",
      "\n",
      "# Iterate through the time stamps\n",
      "for t in sortedTimes:\n",
      "    timeObj = frequencyMap[t]\n",
      "    \n",
      "    # For each tweet, pull the screen name and add it to the list\n",
      "    for tweet in timeObj[\"list\"]:\n",
      "        user = tweet[\"user\"][\"screen_name\"]\n",
      "        \n",
      "        if ( user not in globalUserCounter ):\n",
      "            globalUserCounter[user] = 1\n",
      "            globalUserMap[user] = [tweet]\n",
      "        else:\n",
      "            globalUserCounter[user] += 1\n",
      "            globalUserMap[user].append(tweet)\n",
      "\n",
      "print (\"Unique Users:\", len(globalUserCounter.keys()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sortedUsers = sorted(globalUserCounter, key=globalUserCounter.get, reverse=True)\n",
      "print (\"Top Ten Most Prolific Users:\")\n",
      "for u in sortedUsers[:10]:\n",
      "    print (u, globalUserCounter[u], \"\\n\\t\", \"Random Tweet:\", globalUserMap[u][0][\"text\"], \"\\n----------\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Twitter API\n",
      "\n",
      "It's difficult to see who these people are, but we can go back to the Twitter API and get user descriptions for more information."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tweepy\n",
      "\n",
      "consumer_key = \"\"\n",
      "consumer_secret = \"\"\n",
      "access_token = \"\"\n",
      "access_token_secret = \"\"\n",
      "\n",
      "# Set up the authorization mechanisms for Tweepy to access Twitter's API\n",
      "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
      "auth.secure = True\n",
      "auth.set_access_token(access_token, access_token_secret)\n",
      "\n",
      "api = tweepy.API(auth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print (\"Top Ten Most Prolific Users:\")\n",
      "for u in sortedUsers[:10]:\n",
      "    print (u, globalUserCounter[u])\n",
      "\n",
      "    # Get user info\n",
      "    try:\n",
      "        user = api.get_user(u)\n",
      "        print (\"\\tDescription:\", user.description)\n",
      "    except Exception as te:\n",
      "        print (\"\\tDescription Error:\", te)\n",
      "        \n",
      "    print (\"----------\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Distribution of Postings\n",
      "\n",
      "It appears a few users were posting to Twitter a lot. \n",
      "But how often did *most* Twitter users tweet during this time? \n",
      "We can build a histogram to see this distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(16,8))\n",
      "    \n",
      "# the histogram of the data\n",
      "plt.hist(\n",
      "    [globalUserCounter[x] for x in globalUserCounter], \n",
      "    bins=100, \n",
      "    normed=0, \n",
      "    alpha=0.75,\n",
      "    label=\"Counts\",\n",
      "    log=True)\n",
      "\n",
      "plt.xlabel('Number of Tweets')\n",
      "plt.ylabel('Counts')\n",
      "plt.title(\"Histogram of Frequency\")\n",
      "plt.grid(True)\n",
      "plt.legend()\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Average Number of Posts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "avgPostCount = np.mean([globalUserCounter[x] for x in globalUserCounter])\n",
      "print(\"Average Number of Posts:\", avgPostCount)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Popular Hashtags\n",
      "\n",
      "Hashtags give us a quick way to view the conversation and see what people are discussing. \n",
      "Getting the most popular hashtags is just as easy as getting the most prolific users."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A map for hashtag counts\n",
      "hashtagCounter = {}\n",
      "\n",
      "# For each minute, pull the list of hashtags and add to the counter\n",
      "for t in sortedTimes:\n",
      "    timeObj = frequencyMap[t]\n",
      "    \n",
      "    for tweet in timeObj[\"list\"]:\n",
      "        hashtagList = tweet[\"entities\"][\"hashtags\"]\n",
      "        \n",
      "        for hashtagObj in hashtagList:\n",
      "            \n",
      "            # We lowercase the hashtag to avoid duplicates (e.g., #MikeBrown vs. #mikebrown)\n",
      "            hashtagString = hashtagObj[\"text\"].lower()\n",
      "            \n",
      "            if ( hashtagString not in hashtagCounter ):\n",
      "                hashtagCounter[hashtagString] = 1\n",
      "            else:\n",
      "                hashtagCounter[hashtagString] += 1\n",
      "\n",
      "print (\"Unique Hashtags:\", len(hashtagCounter.keys()))\n",
      "sortedHashtags = sorted(hashtagCounter, key=hashtagCounter.get, reverse=True)\n",
      "print (\"Top Twenty Hashtags:\")\n",
      "for ht in sortedHashtags[:20]:\n",
      "    print (\"\\t\", \"#\" + ht, hashtagCounter[ht])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Event Detection w/ Keyword Frequency\n",
      "\n",
      "Twitter is good for breaking news. When an impactful event occurs, we often see a spike on Twitter of the usage of a related keyword. Some examples are below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# What keywords are we interested in?\n",
      "targetKeywords = [\"obama\", \"tear gas\"]\n",
      "# targetKeywords.append(\"lowery\")\n",
      "# targetKeywords.append(\"reilly\")\n",
      "# targetKeywords.append(\"antonio\")\n",
      "\n",
      "# Build an empty map for each keyword we are seaching for\n",
      "targetCounts = {x:[] for x in targetKeywords}\n",
      "totalCount = []\n",
      "\n",
      "# For each minute, pull the tweet text and search for the keywords we want\n",
      "for t in sortedTimes:\n",
      "    timeObj = frequencyMap[t]\n",
      "    \n",
      "    # Temporary counter for this minute\n",
      "    localTargetCounts = {x:0 for x in targetKeywords}\n",
      "    localTotalCount = 0\n",
      "    \n",
      "    for tweetObj in timeObj[\"list\"]:\n",
      "        tweetString = tweetObj[\"text\"].lower()\n",
      "\n",
      "        localTotalCount += 1\n",
      "        \n",
      "        # Add to the counter if the target keyword is in this tweet\n",
      "        for keyword in targetKeywords:\n",
      "            if ( keyword in tweetString ):\n",
      "                localTargetCounts[keyword] += 1\n",
      "                \n",
      "    # Add the counts for this minute to the main counter\n",
      "    totalCount.append(localTotalCount)\n",
      "    for keyword in targetKeywords:\n",
      "        targetCounts[keyword].append(localTargetCounts[keyword])\n",
      "        \n",
      "# Now plot the total frequency and frequency of each keyword\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(18.5,10.5)\n",
      "\n",
      "plt.title(\"Tweet Frequency\")\n",
      "plt.xticks(smallerXTicks, [sortedTimes[x] for x in smallerXTicks], rotation=90)\n",
      "\n",
      "ax.plot(range(len(frequencyMap)), totalCount, label=\"Total\")\n",
      "\n",
      "for keyword in targetKeywords:\n",
      "    ax.plot(range(len(frequencyMap)), targetCounts[keyword], label=keyword)\n",
      "ax.legend()\n",
      "ax.grid(b=True, which=u'major')\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Language Distribution\n",
      "\n",
      "The protests in Ferguson, MO became an international topic of discussion. As a result, people all over the world were tweeting about the events. Using Twitter's data, we can see how many people were tweeting in different languages."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A map for counting each language\n",
      "languageCounter = {}\n",
      "\n",
      "for t in sortedTimes:\n",
      "    timeObj = frequencyMap[t]\n",
      "    \n",
      "    for tweet in timeObj[\"list\"]:\n",
      "        lang = tweet[\"lang\"]\n",
      "        \n",
      "        if ( lang not in languageCounter ):\n",
      "            languageCounter[lang] = 1\n",
      "        else:\n",
      "            languageCounter[lang] += 1\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "languages = sorted(languageCounter.keys(), key=languageCounter.get, reverse=True)\n",
      "\n",
      "for l in languages:\n",
      "    print (l, languageCounter[l])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(16,8))\n",
      "    \n",
      "# the histogram of the data\n",
      "plt.bar(\n",
      "    np.arange(len(languages)),\n",
      "    [languageCounter[x] for x in languages],\n",
      "    log=True)\n",
      "\n",
      "plt.xticks(np.arange(len(languages)) + 0.5, languages)\n",
      "plt.xlabel('Languages')\n",
      "plt.ylabel('Counts (Log)')\n",
      "plt.title(\"Language Frequency\")\n",
      "plt.grid(True)\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "# Module 2: Geolocation in Twitter\n",
      "\n",
      "Twitter allows users to share their GPS locations when tweeting, but only about 2% of tweets have this information. We can extract this geospatial data to look at patterns in different locations. \n",
      "\n",
      "In this module, we will look at: \n",
      "- Filtering Twitter data with GPS data\n",
      "- Plotting GPS Data\n",
      "- Splitting data based on location\n",
      "- Top users inside/outside Ferguson\n",
      "- Top hashtags inside/outside Ferguson"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Filtering GPS Data\n",
      "\n",
      "Each tweet has a field called \"coordinates\" describing from where the tweet was posted. The field might be null if the tweet contains no location data, or it could contain bounding box information, place information, or GPS coordinates in the form of (longitude, latitude). We want tweets with this GPS data.\n",
      "\n",
      "For more information on tweet JSON formats, check out https://dev.twitter.com/overview/api/tweets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A frequency map for timestamps to geo-coded tweets\n",
      "geoFrequencyMap = {}\n",
      "geoCount = 0\n",
      "\n",
      "# Save only those tweets with tweet['coordinate']['coordinate'] entity\n",
      "for t in sortedTimes:\n",
      "    geos = list(filter(lambda tweet: tweet[\"coordinates\"] != None and \"coordinates\" in tweet[\"coordinates\"], frequencyMap[t][\"list\"]))\n",
      "    geoCount += len(geos)\n",
      "    \n",
      "    # Add to the timestamp map\n",
      "    geoFrequencyMap[t] = {\"count\": len(geos), \"list\": geos}\n",
      "\n",
      "print (\"Number of Geo Tweets:\", geoCount)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### GPS Frequency\n",
      "\n",
      "What is the frequency of GPS-coded tweets?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(18.5,10.5)\n",
      "\n",
      "plt.title(\"Geo Tweet Frequency\")\n",
      "\n",
      "postFreqList = [geoFrequencyMap[x][\"count\"] for x in sortedTimes]\n",
      "\n",
      "smallerXTicks = range(0, len(sortedTimes), 30)\n",
      "plt.xticks(smallerXTicks, [sortedTimes[x] for x in smallerXTicks], rotation=45)\n",
      "\n",
      "ax.plot(range(len(geoFrequencyMap)), [x if x > 0 else 0 for x in postFreqList], color=\"blue\", label=\"Posts\")\n",
      "ax.grid(b=True, which=u'major')\n",
      "ax.legend()\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Plotting GPS Data\n",
      "\n",
      "Now that we have a list of all the tweets with GPS coordinates, we can plot from where in the world these tweets were posted. \n",
      "To make this plot, we can leverage the Basemap package to make a map of the world and convert GPS coordinates to *(x, y)* coordinates we can then plot."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib\n",
      "\n",
      "from mpl_toolkits.basemap import Basemap\n",
      "\n",
      "# Create a list of all geo-coded tweets\n",
      "tmpGeoList = [geoFrequencyMap[t][\"list\"] for t in sortedTimes]\n",
      "geoTweets = reduce(lambda x, y: x + y, tmpGeoList)\n",
      "\n",
      "# For each geo-coded tweet, extract its GPS coordinates\n",
      "geoCoord = [x[\"coordinates\"][\"coordinates\"] for x in geoTweets]\n",
      "\n",
      "# Now we build a map of the world using Basemap\n",
      "land_color = 'lightgray'\n",
      "water_color = 'lightblue'\n",
      "\n",
      "fig, ax = plt.subplots(figsize=(24,24))\n",
      "worldMap = Basemap(projection='merc', llcrnrlat=-80, urcrnrlat=80,\n",
      "                   llcrnrlon=-180, urcrnrlon=180, resolution='l')\n",
      "\n",
      "worldMap.fillcontinents(color=land_color, lake_color=water_color, zorder=1)\n",
      "worldMap.drawcoastlines()\n",
      "worldMap.drawparallels(np.arange(-90.,120.,30.))\n",
      "worldMap.drawmeridians(np.arange(0.,420.,60.))\n",
      "worldMap.drawmapboundary(fill_color=water_color, zorder=0)\n",
      "ax.set_title('World Tweets')\n",
      "\n",
      "# Convert points from GPS coordinates to (x,y) coordinates\n",
      "convPoints = [worldMap(p[0], p[1]) for p in geoCoord]\n",
      "x = [p[0] for p in convPoints]\n",
      "y = [p[1] for p in convPoints]\n",
      "worldMap.scatter(x, y, s=100, marker='x', color=\"red\", zorder=2)\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Filtering By Location\n",
      "\n",
      "We can even use existing Geographic Information System (GIS) tools to determine from where a tweet was posted.\n",
      "For example, we could ask whether a particular tweet\u00a0was posted from the United States. \n",
      "\n",
      "To make this determination, we can use geocoding services like Google Maps, or we can use GIS data files called __Shape Files__, which contain geometric information for a variety of geographic entities (e.g., lakes, roads, county lines, states, countries, etc.).\n",
      "\n",
      "For our purposes, we pulled a shape file containing the county borders for the state of Missouri, which were sourced from the US Census Department (http://www.census.gov/cgi-bin/geo/shapefiles2010/layers.cgi). \n",
      "\n",
      "The first step then is to read in this shape file. To divide the Twitter data into those from inside Ferguson, MO and those outside, we found the county containing Ferguson, and we extract the shapes for that county."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a new map to hold the shape file data\n",
      "stLouisMap = Basemap(llcrnrlon=-130, llcrnrlat=22, urcrnrlon=-64,\n",
      "                     urcrnrlat=52, projection='merc', lat_1=33, lat_2=45,\n",
      "                     lon_0=-95, resolution='i', area_thresh=10000)\n",
      "\n",
      "# Read in the shape file\n",
      "moStateShapeFile = os.path.join(\"data_files\", \"moCountyShapes\", \"tl_2010_29_county10\")\n",
      "shp_info = stLouisMap.readshapefile(moStateShapeFile, 'states', drawbounds=True)\n",
      "\n",
      "# Find only those polygons that describe St. Louis county\n",
      "stLouisCountyPolygons = []\n",
      "for (shapeDict, shape) in zip(stLouisMap.states_info, stLouisMap.states):\n",
      "    if (shapeDict[\"NAME10\"] == \"St. Louis\"):\n",
      "        stLouisCountyPolygons.append(matplotlib.patches.Polygon(shape))\n",
      "        \n",
      "print (\"Shape Count:\", len(stLouisCountyPolygons))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For each tweet, we can check whether its GPS coordinates came from St. Louis county or not."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Maps of timestamps to tweets for inside/outside Ferguson\n",
      "inStLouisFreqMap = {}\n",
      "outStLouisFreqMap = {}\n",
      "\n",
      "# For each geo-coded tweet, extract coordinates and conver them to the Basemap space\n",
      "for t in sortedTimes:\n",
      "    geos = geoFrequencyMap[t][\"list\"]\n",
      "    convPoints = [(stLouisMap(tw[\"coordinates\"][\"coordinates\"][0], tw[\"coordinates\"][\"coordinates\"][1]), tw) for tw in geos]\n",
      "\n",
      "    # Local counters for this time\n",
      "    inStLouisFreqMap[t] = {\"count\": 0, \"list\": []}\n",
      "    outStLouisFreqMap[t] = {\"count\": 0, \"list\": []}\n",
      "    \n",
      "    # For each point, check if it is within St. Louis county or not\n",
      "    for point in convPoints:\n",
      "        x = point[0][0]\n",
      "        y = point[0][1]\n",
      "\n",
      "        inStLouisFlag = False\n",
      "\n",
      "        for polygon in stLouisCountyPolygons:\n",
      "            if ( polygon.contains_point((x, y)) ):\n",
      "                inStLouisFreqMap[t][\"list\"].append(point[1])\n",
      "                inStLouisFlag = True\n",
      "                break\n",
      "\n",
      "        if ( inStLouisFlag == False ):\n",
      "            outStLouisFreqMap[t][\"list\"].append(point[1])\n",
      "\n",
      "print (\"Tweets in St. Louis:\", np.sum([len(inStLouisFreqMap[t][\"list\"]) for t in sortedTimes]))\n",
      "print (\"Tweets outside St. Louis:\", np.sum([len(outStLouisFreqMap[t][\"list\"]) for t in sortedTimes]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Top Twitter Users Inside/Outside Ferguson\n",
      "\n",
      "Now that we have divided the data based on those who were tweeting from within Ferguson, MO versus those who were outside, we can identify the most prolific users in each group."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Top Users Inside Ferguson"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inStLouisTweets = reduce(lambda x, y: x + y, [inStLouisFreqMap[t][\"list\"] for t in sortedTimes])\n",
      "\n",
      "userCounter = {}\n",
      "userMap = {}\n",
      "\n",
      "for tweet in inStLouisTweets:\n",
      "    user = tweet[\"user\"][\"screen_name\"]\n",
      "\n",
      "    if ( user not in userCounter ):\n",
      "        userCounter[user] = 1\n",
      "        userMap[user] = [tweet]\n",
      "    else:\n",
      "        userCounter[user] += 1\n",
      "        userMap[user].append(tweet)\n",
      "\n",
      "print (\"Unique Users in St. Louis:\", len(userCounter.keys()))\n",
      "sortedUsers = sorted(userCounter, key=userCounter.get, reverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Top Users in Ferguson:\")\n",
      "for u in sortedUsers[:10]:\n",
      "    print (u, userCounter[u])\n",
      "    \n",
      "    # Get user info\n",
      "    try:\n",
      "        user = api.get_user(u)\n",
      "        print (\"\\t\", user.description)\n",
      "    except Exception as te:\n",
      "        print (\"\\t\", te)\n",
      "        \n",
      "    print (\"\\t\", userMap[u][0][\"text\"], \"\\n----------\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Top Users Outside Ferguson"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outStLouisTweets = reduce(lambda x, y: x + y, [outStLouisFreqMap[t][\"list\"] for t in sortedTimes])\n",
      "\n",
      "userCounter = {}\n",
      "userMap = {}\n",
      "\n",
      "for tweet in outStLouisTweets:\n",
      "    user = tweet[\"user\"][\"screen_name\"]\n",
      "\n",
      "    if ( user not in userCounter ):\n",
      "        userCounter[user] = 1\n",
      "        userMap[user] = [tweet]\n",
      "    else:\n",
      "        userCounter[user] += 1\n",
      "        userMap[user].append(tweet)\n",
      "\n",
      "print (\"Unique Users outside St. Louis:\", len(userCounter.keys()))\n",
      "sortedUsers = sorted(userCounter, key=userCounter.get, reverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print (\"Top Ten Most Prolific Users:\")\n",
      "for u in sortedUsers[:10]:\n",
      "    print (u, userCounter[u])\n",
      "    \n",
      "    # Get user info\n",
      "    try:\n",
      "        user = api.get_user(u)\n",
      "        print (\"\\t\", user.description)\n",
      "    except Exception as te:\n",
      "        print (\"\\t\", te)\n",
      "        \n",
      "    print (\"\\t\", userMap[u][0][\"text\"], \"\\n----------\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Hashtags By Location\n",
      "\n",
      "We've already looked at popular hashtags over the course of the day. How does this usage change from inside Ferguson to outside?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Popular Hashtags in Ferguson"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inStlHashtagCounter = {}\n",
      "\n",
      "for tweet in inStLouisTweets:\n",
      "    hashtagList = tweet[\"entities\"][\"hashtags\"]\n",
      "\n",
      "    for hashtagObj in hashtagList:\n",
      "        hashtagString = hashtagObj[\"text\"].lower()\n",
      "\n",
      "        if ( hashtagString not in inStlHashtagCounter ):\n",
      "            inStlHashtagCounter[hashtagString] = 1\n",
      "        else:\n",
      "            inStlHashtagCounter[hashtagString] += 1\n",
      "\n",
      "print (\"Unique Hashtags in Ferguson:\", len(inStlHashtagCounter.keys()))\n",
      "sortedInStlHashtags = sorted(inStlHashtagCounter, key=inStlHashtagCounter.get, reverse=True)\n",
      "print (\"Top Twenty Hashtags in Ferguson:\")\n",
      "for ht in sortedInStlHashtags[:20]:\n",
      "    print (\"\\t\", \"#\" + ht, inStlHashtagCounter[ht])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Popular Hashtags outside Ferguson"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outStlHashtagCounter = {}\n",
      "\n",
      "for tweet in outStLouisTweets:\n",
      "    hashtagList = tweet[\"entities\"][\"hashtags\"]\n",
      "\n",
      "    for hashtagObj in hashtagList:\n",
      "        hashtagString = hashtagObj[\"text\"].lower()\n",
      "\n",
      "        if ( hashtagString not in outStlHashtagCounter ):\n",
      "            outStlHashtagCounter[hashtagString] = 1\n",
      "        else:\n",
      "            outStlHashtagCounter[hashtagString] += 1\n",
      "\n",
      "print (\"Unique Hashtags Outside Ferguson:\", len(outStlHashtagCounter.keys()))\n",
      "sortedOutStlHashtags = sorted(outStlHashtagCounter, key=outStlHashtagCounter.get, reverse=True)\n",
      "print (\"Top Twenty Hashtags Outside Ferguson:\")\n",
      "for ht in sortedOutStlHashtags[:20]:\n",
      "    print (\"\\t\", \"#\" + ht, outStlHashtagCounter[ht])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "# Module 3: Media in Twitter\n",
      "\n",
      "Twitter is excellent for sharing media, either photographs, movies, or links websites. When you share pictures, Twitter stores them and links to them directly. We can use this data to sample some random pictures taken from each hour of the data we have.\n",
      "\n",
      "We'll look at:\n",
      "- Images By Hour\n",
      "- Images Inside Ferguson\n",
      "- Images Outside Ferguson"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Images By Hour\n",
      "\n",
      "First, we need to reduce our map of minutes->tweets to hours->tweets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hourlyInterval = {}\n",
      "\n",
      "for t in sortedTimes:\n",
      "    newTime = t.replace(second=0, minute=0)\n",
      "    \n",
      "    currentTimeObject = frequencyMap[t]\n",
      "    if ( newTime not in hourlyInterval ):\n",
      "        hourlyInterval[newTime] = {\n",
      "            \"count\": currentTimeObject[\"count\"],\n",
      "            \"list\": currentTimeObject[\"list\"]\n",
      "            }\n",
      "    else:\n",
      "        hourlyInterval[newTime][\"count\"] += currentTimeObject[\"count\"]\n",
      "        hourlyInterval[newTime][\"list\"] = hourlyInterval[newTime][\"list\"] + currentTimeObject[\"list\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we filter out retweets and keep only those tweets with a media listing in the \"entities\" section. \n",
      "Then, we select a random image from the list of pictures for that hour and display it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import display\n",
      "from IPython.display import Image\n",
      "\n",
      "for h in sorted(hourlyInterval.keys()):\n",
      "    noRetweets = list(filter(lambda tweet: not tweet[\"text\"].lower().startswith(\"rt\"), hourlyInterval[h][\"list\"]))\n",
      "    tweetsWithMedia = list(filter(lambda tweet: \"media\" in tweet[\"entities\"], noRetweets))\n",
      "    print (h, hourlyInterval[h][\"count\"], len(tweetsWithMedia), )\n",
      "    \n",
      "    randIndex = np.random.random_integers(0, len(tweetsWithMedia)-1, size=1)\n",
      "    imgUrl = tweetsWithMedia[randIndex][\"entities\"][\"media\"][0][\"media_url\"]\n",
      "    display(Image(url=imgUrl))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Pictures from Inside Ferguson, MO\n",
      "\n",
      "We can also extract images people tweeted from Ferguson."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stlTweetsWithMedia = list(filter(lambda tweet: \"media\" in tweet[\"entities\"], inStLouisTweets))\n",
      "print (\"Tweets with Media:\", len(stlTweetsWithMedia))\n",
      "\n",
      "for tweet in stlTweetsWithMedia:\n",
      "    imgUrl = tweet[\"entities\"][\"media\"][0][\"media_url\"]\n",
      "    display(Image(url=imgUrl))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Pictures from Outside Ferguson, MO\n",
      "\n",
      "Here, we extract 10 random images from *outside* Ferguson."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outStlTweetsWithMedia = list(filter(lambda tweet: \"media\" in tweet[\"entities\"], outStLouisTweets))\n",
      "print (\"Tweets outside St. Louis with Media:\", len(outStlTweetsWithMedia))\n",
      "\n",
      "np.random.shuffle(outStlTweetsWithMedia)\n",
      "for tweet in outStlTweetsWithMedia[:10]:\n",
      "    imgUrl = tweet[\"entities\"][\"media\"][0][\"media_url\"]\n",
      "    display(Image(url=imgUrl))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "# Module 4: Sentiment Analysis\n",
      "\n",
      "Another popular type of analysis people do on social networks is \"sentiment analysis,\" which is used to figure out how people **feel** about a specific topic.\n",
      "\n",
      "One way to explore sentiment is to use a list of keywords with tagged sentiment information (e.g., \"happy\" or \"awesome\" might have high sentiment whereas \"terrible\" or \"awful\" might have very low sentiment). Then, we can count the occurrence of these tagged keywords to get a sense of how people feel about the topic at hand.\n",
      "\n",
      "We use the AFINN Sentiment Dictionary for our keyword list. Link here: http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010\n",
      "\n",
      "- Emotive Keywords and Emoticons\n",
      "- Per-Tweet Average Sentiment\n",
      "- Sentiment Over Time\n",
      "- GIS + Sentiment "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "# Read in the sentiment/valence files\n",
      "dataFilePath = os.path.join(\"data_files\", \"SentiStrength\")\n",
      "valenceFile = os.path.join(dataFilePath, \"EmotionLookupTable.txt\")\n",
      "emoticonFile = os.path.join(dataFilePath, \"EmoticonLookupTable.txt\")\n",
      "\n",
      "valenceList = []\n",
      "\n",
      "# Open the valence file and read in each word/valence pair\n",
      "for line in open(valenceFile, \"r\"):\n",
      "    # Split the line based on tabs and select the first two elements\n",
      "    (word, valence) = line.split(\"\\t\")[:2]\n",
      "    \n",
      "    wordRegex = re.compile(word)\n",
      "    valencePair = (wordRegex, int(valence))\n",
      "    valenceList.append(valencePair)\n",
      "    \n",
      "# Open the emoticon file and read in the valence for each emoticon\n",
      "for line in codecs.open(emoticonFile, \"r\", \"utf-8\"):\n",
      "    # Split the line based on tabs and select the first two elements\n",
      "    (emoticon, valence) = line.split(\"\\t\")[:2]\n",
      "    \n",
      "    emoticonRegex = re.compile(re.escape(emoticon))\n",
      "    valencePair = (emoticonRegex, int(valence))\n",
      "    valenceList.append(valencePair)\n",
      "    \n",
      "print (\"Number of Sentiment Keywords:\", len(valenceList))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Examples of sentiment pairs\n",
      "for i in np.random.random_integers(0, len(valenceList)-1, 10):\n",
      "    print(valenceList[i][0].pattern, \"\\t\", valenceList[i][1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate sentiment measures for each time\n",
      "timeSentiments = {}\n",
      "for t in sortedTimes:\n",
      "    \n",
      "    tweetList = frequencyMap[t][\"list\"]\n",
      "    sentimentList = []\n",
      "    thisMinuteSentiment = None\n",
      "    \n",
      "    for tweet in tweetList:\n",
      "        \n",
      "        # Calculate the average sentiment for this tweet\n",
      "        tweetText = tweet[\"text\"].lower()\n",
      "\n",
      "        # skip retweets\n",
      "        if ( tweetText.startswith(\"rt \") ):\n",
      "            continue\n",
      "\n",
      "        valCount = 0\n",
      "        valSum = 0.0\n",
      "        valAvg = 0.0\n",
      "        for valencePair in valenceList:\n",
      "            if ( valencePair[0].search(tweetText) is not None ):\n",
      "                valCount += 1\n",
      "                valSum += valencePair[1]\n",
      "\n",
      "        if ( valCount > 0 ):\n",
      "            valAvg = valSum / valCount\n",
      "            sentimentList.append(valAvg)\n",
      "    \n",
      "    if ( len(sentimentList) > 0 ):\n",
      "        thisMinuteSentiment = np.array(sentimentList).mean()\n",
      "    else:\n",
      "        thisMinuteSentiment = 0.0\n",
      "        \n",
      "    timeSentiments[t] = thisMinuteSentiment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(18.5,10.5)\n",
      "\n",
      "plt.title(\"Sentiment Over Time\")\n",
      "\n",
      "postFreqList = [frequencyMap[x][\"count\"] for x in sortedTimes]\n",
      "sentList = [timeSentiments[x] for x in sortedTimes]\n",
      "\n",
      "smallerXTicks = range(0, len(sortedTimes), 30)\n",
      "plt.xticks(smallerXTicks, [sortedTimes[x] for x in smallerXTicks], rotation=90)\n",
      "\n",
      "ax.plot(range(len(frequencyMap)), [x if x > 0 else 0 for x in postFreqList], color=\"blue\", label=\"Posts\")\n",
      "\n",
      "ax2 = ax.twinx()\n",
      "ax2.plot([0], [0], color=\"blue\", label=\"Posts\")\n",
      "ax2.plot(range(len(frequencyMap)), sentList, color=\"green\", label=\"Sentiment\")\n",
      "ax2.set_ylim(-6,6)\n",
      "\n",
      "ax.grid(b=True, which=u'major')\n",
      "ax2.legend()\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Based on this data, we can see that most people are pretty unhappy with the events in Ferguson, MO. This result is not all that unexpected."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## GIS + Sentiment"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(18.5,10.5)\n",
      "\n",
      "plt.title(\"Sentiment Histrogram\")\n",
      "\n",
      "for (loc, (tweetList, color)) in {\"Inside\": (inStLouisTweets, \"green\"), \"Outside\": (outStLouisTweets, \"blue\")}.items():\n",
      "\n",
      "    localSentimentList = []\n",
      "    for tweet in tweetList:\n",
      "\n",
      "        # Calculate the average sentiment for this tweet\n",
      "        tweetText = tweet[\"text\"].lower()\n",
      "\n",
      "        # skip retweets\n",
      "        if ( tweetText.startswith(\"rt \") ):\n",
      "            continue\n",
      "\n",
      "        valCount = 0\n",
      "        valSum = 0.0\n",
      "        valAvg = 0.0\n",
      "        for valencePair in valenceList:\n",
      "            if ( valencePair[0].search(tweetText) is not None ):\n",
      "                valCount += 1\n",
      "                valSum += valencePair[1]\n",
      "\n",
      "        if ( valCount > 0 ):\n",
      "            valAvg = valSum / valCount\n",
      "            localSentimentList.append(valAvg)\n",
      "\n",
      "    print(\"Number of Sentiment Tweets:\", len(localSentimentList))\n",
      "\n",
      "    ax.hist(localSentimentList, range=(-5, 5), normed=True, alpha=0.5, color=color, label=loc)\n",
      "\n",
      "ax.grid(b=True, which=u'major')\n",
      "ax.legend()\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "# Module 5: Topic Modeling\n",
      "\n",
      "Along with sentiment analysis, a question often asked of social networks is \"What are people talking about?\" \n",
      "We can answer this question using tools from topic modeling and natural language processing, and we can even divide this data to see what people in Ferguson are talking about versus those outside.\n",
      "\n",
      "To generate these topic models, we will use the Gensim package's implementation of Latent Dirichlet Allocation (LDA), which basically constructs a set of topics where each topic is described as a probability distribution over the words in our tweets. Several other methods for topic modeling exist as well.\n",
      "\n",
      "- Topics Across Twitter\n",
      "- Topics in Ferguson\n",
      "- Topics outside Ferguson"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gensim.models.ldamodel\n",
      "import gensim.matutils\n",
      "import sklearn.cluster\n",
      "import sklearn.feature_extraction \n",
      "import sklearn.feature_extraction.text\n",
      "import sklearn.metrics\n",
      "import sklearn.preprocessing\n",
      "\n",
      "from nltk.corpus import stopwords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We first extract the text of all English tweets that are *not* retweets and make the text lowercase."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "enFilter = lambda x: True if x[\"lang\"] == \"en\" else False\n",
      "\n",
      "# Get all tweets, filter out retweets, save only those in English, and conver to lowercase\n",
      "allTweetList = reduce(lambda x, y: x + y, [frequencyMap[t][\"list\"] for t in sortedTimes])\n",
      "noRetweetsList = list(filter(lambda x: not x[\"text\"].lower().startswith(\"rt\"), allTweetList))\n",
      "onlyEnglishTweets = list(filter(enFilter, noRetweetsList))\n",
      "lowerTweetText = [x[\"text\"].lower() for x in onlyEnglishTweets]\n",
      "\n",
      "print (\"All Tweet Count:\", len(allTweetList))\n",
      "print (\"Reduced Tweet Count:\", len(lowerTweetText))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we build a list of stop words (words we don't care about) and build a feature generator (the vectorizer) that assigns integer keys to tokens and counts the number of each token."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "enStop = stopwords.words('english')\n",
      "\n",
      "# Skip stop words, retweet signs, @ symbols, and URL headers\n",
      "stopList = enStop + [\"http\", \"https\", \"rt\", \"@\", \":\"]\n",
      "\n",
      "vectorizer = sklearn.feature_extraction.text.CountVectorizer(strip_accents='unicode', \n",
      "                                                             tokenizer=None,\n",
      "                                                             token_pattern='(?u)#?\\\\b\\\\w+[\\'-]?\\\\w+\\\\b',\n",
      "                                                             stop_words=stopList,\n",
      "                                                             binary=True)\n",
      "# Create a vectorizer for all our content\n",
      "vectorizer.fit(lowerTweetText)\n",
      "\n",
      "# Get all the words in our text\n",
      "names = vectorizer.get_feature_names()\n",
      "\n",
      "# Create a map for vectorizer IDs to words\n",
      "id2WordDict = dict(zip(range(len(vectorizer.get_feature_names())), names))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Topics Across Twitter\n",
      "\n",
      "We then use the vectorizer to transform our tweet text into a feature set, which essentially is a table with rows of tweets, columns for each keyword, and each cell is the number of times that keyword appears in that tweet.\n",
      "\n",
      "We then convert that table into a model the Gensim package can handle, apply LDA, and grab the top 10 topics, 10 words that describe that topic, and print them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a corpus for \n",
      "corpus = vectorizer.transform(lowerTweetText)\n",
      "gsCorpus = gensim.matutils.Sparse2Corpus(corpus, documents_columns=False)\n",
      "# lda = gensim.models.ldamodel.LdaModel(gsCorpus, id2word=id2WordDict, num_topics=10)\n",
      "lda = gensim.models.LdaMulticore(gsCorpus, id2word=id2WordDict, num_topics=100, passes=2)\n",
      "\n",
      "ldaTopics = lda.show_topics(num_topics=10, num_words=10, formatted=False)\n",
      "topicTokens = [[token for (_,token) in topic] for topic in ldaTopics]\n",
      "for i in range(len(topicTokens)):\n",
      "    print (\"Topic:\", i)\n",
      "    for token in topicTokens[i]:\n",
      "        print (\"\\t\", token)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Topics Inside Ferguson\n",
      "\n",
      "We do the same thing with only those tweets in Ferguson to find topics people are discussing there."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inStlLowerTweetText = [x[\"text\"].lower() for x in filter(enFilter, inStLouisTweets)]\n",
      "\n",
      "corpus = vectorizer.transform(inStlLowerTweetText)\n",
      "gsCorpus = gensim.matutils.Sparse2Corpus(corpus, documents_columns=False)\n",
      "lda = gensim.models.ldamulticore.LdaMulticore(gsCorpus, id2word=id2WordDict, num_topics=10, passes=10)\n",
      "\n",
      "ldaTopics = lda.show_topics(num_topics=10, num_words=10, formatted=False)\n",
      "topicTokens = [[token for (_,token) in topic] for topic in ldaTopics]\n",
      "for i in range(len(topicTokens)):\n",
      "    print (\"Topic:\", i)\n",
      "    for token in topicTokens[i]:\n",
      "        print (\"\\t\", token)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Topics Outside Ferguson"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outStlLowerTweetText = [x[\"text\"].lower() for x in filter(enFilter, outStLouisTweets)]\n",
      "\n",
      "corpus = vectorizer.transform(outStlLowerTweetText)\n",
      "gsCorpus = gensim.matutils.Sparse2Corpus(corpus, documents_columns=False)\n",
      "lda = gensim.models.ldamulticore.LdaMulticore(gsCorpus, id2word=id2WordDict, num_topics=50, passes=10)\n",
      "\n",
      "ldaTopics = lda.show_topics(num_topics=10, num_words=10, formatted=False)\n",
      "topicTokens = [[token for (_,token) in topic] for topic in ldaTopics]\n",
      "for i in range(len(topicTokens)):\n",
      "    print (\"Topic:\", i)\n",
      "    for token in topicTokens[i]:\n",
      "        print (\"\\t\", token)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "# Module 6: Network Analysis\n",
      "\n",
      "Issues of race, class, poverty, and police militarization all came out during the protests and clashes with law enforcement, and it didn't take much to find people on either side of each issue on Twitter. \n",
      "At the same time, people were turning to Twitter for news about the events on the ground since many perceived that mainstream media wasn't giving the events adequate or fair coverage. \n",
      "Using network analysis, we can get some idea about who the most important Twitter users were during this time, and how people split into groups online.\n",
      "\n",
      "For this analysis, we'll use the NetworkX package to construct a social graph of how people interact. Each person in our Twitter data will be a node in our graph, and edges in the graph will represent mentions during this timeframe.\n",
      "Then we will explore a few simple analytical methods in network analysis, including:\n",
      "- Graph Building\n",
      "- User Centrality\n",
      "- Network Visualization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Graph Building\n",
      "\n",
      "To limit the amount of data we're looking at, we'll only build the network for people who have GPS locations in their tweets and the people they mention. \n",
      "We build this network simply by iterating through all the tweets in our GPS list and extract the \"user_mentions\" list from the \"entities\" section of the tweet object.\n",
      "For each mention a user makes, we will add an edge from that user to the user he/she mentioned.\n",
      "\n",
      "In addition, we will append a location attribute to each user based on whether we saw them in Ferguson or outside of Ferguson."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import networkx as nx\n",
      "\n",
      "graph = nx.DiGraph()\n",
      "\n",
      "geoCodedMap = {1: inStLouisTweets, 0: outStLouisTweets}\n",
      "\n",
      "for (location, locationList) in geoCodedMap.items():\n",
      "    print (location, len(locationList))\n",
      "    \n",
      "    for tweet in locationList:\n",
      "        userName = tweet[\"user\"][\"screen_name\"]\n",
      "        graph.add_node(userName, loc=location)\n",
      "\n",
      "        mentionList = tweet[\"entities\"][\"user_mentions\"]\n",
      "        \n",
      "        for otherUser in mentionList:\n",
      "            otherUserName = otherUser[\"screen_name\"]\n",
      "            if ( graph.has_node(otherUserName) == False ):\n",
      "                graph.add_node(otherUserName, loc=-1)\n",
      "            graph.add_edge(userName, otherUserName)\n",
      "        \n",
      "print (\"Number of Users:\", len(graph.node))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Central Users\n",
      "\n",
      "In network analysis, \"centrality\" is used to measure the importance of a given node. \n",
      "Many different types of centrality are used to describe various types of importance though.\n",
      "Examples include \"closeness centrality,\" which measures how close a node is to all other nodes in the network, versus \"betweeness centrality,\" which measures how many shortest paths run through the given node.\n",
      "Nodes with high closeness centrality are important for rapidly disseminating information or spreading disease, whereas nodes with high betweeness are more important to ensure the network stays connected.\n",
      "\n",
      "The PageRank is another algorithm for measuring importance and was proposed by Sergey Brin and Larry Page for the early version of Google's search algorithm.\n",
      "NetworkX has an implementation of the PageRank algorithm that we can use to look at the most important/authoritative users on Twitter based on their connections to other users."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pageRankList = nx.pagerank_numpy(graph)\n",
      "\n",
      "highRankNodes = sorted(pageRankList.keys(), key=pageRankList.get, reverse=True)\n",
      "for x in highRankNodes[:20]:\n",
      "    user = api.get_user(x)\n",
      "    print (x, pageRankList[x], \"\\n\\t\", user.description, \"\\n----------\")\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Network Visualization\n",
      "\n",
      "A lot of information can be gleaned from visualizing how these networks interact. In Python, we can plot these networks relatively easily. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print (len(graph.nodes(data=True)))\n",
      "\n",
      "colors = [0.9 if x[1][\"loc\"] == 1 else 0.1 for x in graph.nodes(data=True)]\n",
      "pos = {x:(np.random.rand(2) * 10) for x in graph.nodes()}\n",
      "nx.draw_networkx_nodes(graph, pos, node_color=colors)\n",
      "nx.draw_networkx_edges(graph, pos)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This graph is relatively uninformative, so we will turn to other tools for better visualization.\n",
      "\n",
      "We first save this graph to a file, so we can import into other tools."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nx.write_graphml(graph, \"inVsOutNetwork.graphml\", encoding='utf-8', prettyprint=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# NodeXL Demo"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# If you want to play with the full graph, \n",
      "# here is code that will build it up for you.\n",
      "# Be careful. It's large.\n",
      "\n",
      "fullGraph = nx.DiGraph()\n",
      "\n",
      "inStlUsers = set(map(lambda x: x[\"user\"][\"screen_name\"], inStLouisTweets))\n",
      "outStlUsers = set(map(lambda x: x[\"user\"][\"screen_name\"], outStLouisTweets))\n",
      "\n",
      "for (userName, tweetList) in globalUserMap.items():\n",
      "    \n",
      "    location = -1\n",
      "    if ( userName in inStlUsers ):\n",
      "        location = 1\n",
      "    elif (userName in outStlUsers ):\n",
      "        location = 0\n",
      "        \n",
      "    fullGraph.add_node(userName, loc=location)\n",
      "\n",
      "    for tweet in tweetList:\n",
      "        mentionList = tweet[\"entities\"][\"user_mentions\"]\n",
      "\n",
      "        for otherUser in mentionList:\n",
      "            otherUserName = otherUser[\"screen_name\"]\n",
      "            if ( fullGraph.has_node(otherUserName) == False ):\n",
      "                fullGraph.add_node(otherUserName, loc=-1)\n",
      "            fullGraph.add_edge(userName, otherUserName)\n",
      "            \n",
      "print (\"Number of Users:\", len(fullGraph.node))\n",
      "\n",
      "nx.write_graphml(fullGraph, \"fullNetwork.graphml\", encoding='utf-8', prettyprint=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}